{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimentMA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfJWpVun2K3A",
        "outputId": "c0248605-6d89-4ac6-86b8-3a740d6c5350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "from transformers import pipeline\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from pandas import DataFrame, concat\n",
        "import os\n",
        "\n",
        "\n",
        "def preprocess(text_file):\n",
        "    \n",
        "    book = text_file\n",
        "    book = re.split('\\d+\\.', book)\n",
        "    bookch=list()\n",
        "    text=list()\n",
        "    for index, chapter in enumerate(book, start=0):\n",
        "        bookch.append(index)\n",
        "        text.append(chapter)\n",
        "    return text #for paragraph in text\n",
        "\n",
        "\n",
        "def SentANAL(text_file_path):\n",
        "  MODEL =f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "  config = AutoConfig.from_pretrained(MODEL)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "  model.save_pretrained(MODEL)\n",
        "  tokenizer.save_pretrained(MODEL)\n",
        "  config.save_pretrained(MODEL)\n",
        "\n",
        "  text_file = open(text_file_path,\"r\",encoding='UTF8').read()\n",
        "  text = preprocess(text_file)\n",
        "\n",
        "  fragment_sa=[] #where the output will be stored\n",
        "  for fragment in text:  ##ITERATION -- apply to each fragment from our list##\n",
        "\n",
        "    encoded_input = tokenizer(fragment, padding='max_length', truncation=True,max_length=511, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    fragment_sa.append(scores)\n",
        "\n",
        "  # Print labels and scores --here they are rounded up, but it includes labels and i don't like labels\n",
        "  # scores=[]\n",
        "  # ranking = np.argsort(scores)\n",
        "  # ranking = ranking[::-1]\n",
        "  # for i in range(scores.shape[0]):\n",
        "  #     l = config.id2label[ranking[i]]\n",
        "  #     s = scores[ranking[i]]\n",
        "  #     scores_final =f\"{i+1}) {l} {np.round(float(s), 4)}\"\n",
        "  #     scores_l.append(scores_final)\n",
        "\n",
        "  sentiment_df = DataFrame(np.vstack(fragment_sa),columns=['Negative','Neutral','Positive'])\n",
        "\n",
        "  return sentiment_df\n",
        "\n",
        "def create_df(text_file_path,book_number,sa_to_merge):\n",
        "    book = open(text_file_path,\"r\",encoding='UTF8').read()\n",
        "    book_l=re.split('\\d+\\.', book)\n",
        "    bookch=list()\n",
        "    text=list()\n",
        "    for index, chapter in enumerate(book_l, start=0):\n",
        "        bookch.append(index)\n",
        "        text.append(chapter)\n",
        "    data={'Book': book_number, 'BookChapter':bookch}\n",
        "    book_df=DataFrame(data,index=None)\n",
        "    book_SA_df=concat([book_df,sa_to_merge],axis=1)\n",
        "    book_SA_df=book_SA_df.iloc[1: , :]\n",
        "    book_SA_df.reset_index(drop=True)\n",
        "    return book_SA_df\n",
        "\n",
        "\n",
        "\n",
        "def store_csv(df_to_store,file_path,file_name):\n",
        "\n",
        "\n",
        "  f_name= file_name\n",
        "  df_to_store.to_csv(os.path.join(file_path, (f_name+'.csv')))\n",
        "  return True\n",
        "\n",
        "\n",
        "\n",
        "text_file_path='/content/drive/MyDrive/DHDK/SA_Meditations/SA/MAtext/MeditationsBook12.txt'\n",
        "sentiment_df= SentANAL(text_file_path)\n",
        "book1_sa_df= create_df(text_file_path, \"12\", sentiment_df)\n",
        "print(store_csv(book1_sa_df,'/content/drive/MyDrive/DHDK/SA_Meditations','SA_df_book12'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKGrE0FQsCcL",
        "outputId": "62aad52b-4067-4d28-c701-57dc538ce9ad"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}